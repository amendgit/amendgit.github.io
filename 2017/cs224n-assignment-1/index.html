<!DOCTYPE html>
<html>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="1 Softmax (10 points)(a) (5 points) Prove that softmax is invariant to constant offsets in the input, that is, for any input vector x and any constant c, $$softmax(x) = softmax(x + c)$$ where x + c me">
<meta name="keywords" content="cs224n,machine learning">
<meta property="og:type" content="article">
<meta property="og:title" content="CS224N Assignment 1">
<meta property="og:url" content="http://www.amendgit.com/2017/cs224n-assignment-1/index.html">
<meta property="og:site_name" content="amendgit&#39;s blog">
<meta property="og:description" content="1 Softmax (10 points)(a) (5 points) Prove that softmax is invariant to constant offsets in the input, that is, for any input vector x and any constant c, $$softmax(x) = softmax(x + c)$$ where x + c me">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://www.amendgit.com/media/cs224n-assignment-1/one-hidden-layer-nn.png">
<meta property="og:image" content="http://www.amendgit.com/media/q3_word_vectors-2.png">
<meta property="og:image" content="http://www.amendgit.com/media/q4_reg_v_acc.png">
<meta property="og:image" content="http://www.amendgit.com/media/q4_dev_conf.png">
<meta property="og:updated_time" content="2017-07-26T12:11:05.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS224N Assignment 1">
<meta name="twitter:description" content="1 Softmax (10 points)(a) (5 points) Prove that softmax is invariant to constant offsets in the input, that is, for any input vector x and any constant c, $$softmax(x) = softmax(x + c)$$ where x + c me">
<meta name="twitter:image" content="http://www.amendgit.com/media/cs224n-assignment-1/one-hidden-layer-nn.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>CS224N Assignment 1</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- rss -->
    
    
</head>

<body>
    
      <div id="header-post">
  <a id="menu-icon-tablet" href="#"><i class="fa fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="http://github.com/amendgit">Projects</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        <li><a class="icon" href="#"><i class="fa fa-bars fa-lg" aria-hidden="true" onmouseover='$("#i-toc").toggle();' onmouseout='$("#i-toc").toggle();' onclick='$("#toc").toggle();return false;'></i></a></li>
        
        <li><a class="icon" href="/2017/leetcode-solutions-051-nnn/"><i class="fa fa-chevron-left" aria-hidden="true" onmouseover='$("#i-prev").toggle();' onmouseout='$("#i-prev").toggle();'></i></a></li>
        
        
        <li><a class="icon" href="/2017/linux-understand-process/"><i class="fa fa-chevron-right" aria-hidden="true" onmouseover='$("#i-next").toggle();' onmouseout='$("#i-next").toggle();'></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa fa-chevron-up" aria-hidden="true" onmouseover='$("#i-top").toggle();' onmouseout='$("#i-top").toggle();'></i></a></li>
        <li><a class="icon" href="#"><i class="fa fa-share-alt" aria-hidden="true" onmouseover='$("#i-share").toggle();' onmouseout='$("#i-share").toggle();' onclick='$("#share").toggle();return false;'></i></a></li>
      </ul>
      <span id="i-toc" class="info" style="display:none;">Table of content</span>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://www.amendgit.com/2017/cs224n-assignment-1/"><i class="fa fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://www.amendgit.com/2017/cs224n-assignment-1/&text=CS224N Assignment 1"><i class="fa fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://www.amendgit.com/2017/cs224n-assignment-1/&title=CS224N Assignment 1"><i class="fa fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://www.amendgit.com/2017/cs224n-assignment-1/&is_video=false&description=CS224N Assignment 1"><i class="fa fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=CS224N Assignment 1&body=Check out this article: http://www.amendgit.com/2017/cs224n-assignment-1/"><i class="fa fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://www.amendgit.com/2017/cs224n-assignment-1/&title=CS224N Assignment 1"><i class="fa fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://www.amendgit.com/2017/cs224n-assignment-1/&title=CS224N Assignment 1"><i class="fa fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://www.amendgit.com/2017/cs224n-assignment-1/&title=CS224N Assignment 1"><i class="fa fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://www.amendgit.com/2017/cs224n-assignment-1/&title=CS224N Assignment 1"><i class="fa fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://www.amendgit.com/2017/cs224n-assignment-1/&name=CS224N Assignment 1&description="><i class="fa fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Softmax-10-points"><span class="toc-number">1.</span> <span class="toc-text">1 Softmax (10 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Neural-Network-Basics-30-points"><span class="toc-number">2.</span> <span class="toc-text">2 Neural Network Basics (30 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-word2vec-40-points-2-bonus"><span class="toc-number">3.</span> <span class="toc-text">3 word2vec (40 points + 2 bonus)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Sentiment-Analysis-20-points"><span class="toc-number">4.</span> <span class="toc-text">4 Sentiment Analysis (20 points)</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index width mx-auto px2 my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        CS224N Assignment 1
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">amendgit's blog</span>
      </span>
      
    <div class="postdate">
        <time datetime="2017-07-04T11:45:20.000Z" itemprop="datePublished">2017-07-04</time>
    </div>


      
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/cs224n/">cs224n</a>, <a class="tag-link" href="/tags/machine-learning/">machine learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h2 id="1-Softmax-10-points"><a href="#1-Softmax-10-points" class="headerlink" title="1 Softmax (10 points)"></a>1 Softmax (10 points)</h2><p><strong>(a)</strong> (5 points) Prove that softmax is invariant to constant offsets in the input, that is, for any input vector x and any constant c,</p>
<p>$$softmax(x) = softmax(x + c)$$</p>
<p>where x + c means adding the constant c to every dimension of x. Remember that</p>
<p>$$softmax(x)_i = \frac {e^{x_i}}{\sum_je^{x_j}}$$</p>
<p>Note: In practice, we make use of this property and choose c = − maxi xi when computing softmax probabilities for numerical stability (i.e., subtracting its maximum element from all elements of x).</p>
<p><strong>Solution</strong></p>
<p>数学中，softmax函数（或者叫归一化指数函数），是逻辑函数sigmoid的推广。它将由实数元素组成K维的向量z，压缩到由属于区间[0,1]的实数组成的K维向量σ(z)且各项元素之和为1。</p>
<p>概率论中，softmax的函数输出通常被用于表示分类分布（categorical distribution，一种由K种可能的输出组成的离散型概率分布）。</p>
<p>$$<br>\begin{align}<br>softmax(x - c)_i =&amp; \frac {e^{x_i - c}}{\sum_je^{x_j - c}} \<br>    =&amp; \frac {e^{x_i} e^{-c}}{\sum_je^{x_j} e^{-c}} \<br>    =&amp; \frac {e^{x_i} e^{-c}}{(\sum_je^{x_j}) e^{-c}} \<br>    =&amp; \frac {e^{x_i}}{\sum_je^{x_j}}<br>\end{align}<br>$$ </p>
<p><strong>(b)</strong> (5 points) Given an input matrix of N rows and D columns, compute the softmax prediction for each row using the optimization in part (a). Write your implementation in q1_softmax.py. You may test by executing python q1_softmax.py.<br>Note: The provided tests are not exhaustive. Later parts of the assignment will reference this code so it is important to have a correct implementation. Your implementation should also be efficient and vectorized whenever possible (i.e., use numpy matrix operations rather than for loops). A non-vectorized implementation will not receive full credit!</p>
<p><strong>Solution</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Compute the softmax function for each row of the input x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It is crucial that this function is optimized for speed because</span></span><br><span class="line"><span class="string">    it will be used frequently in later code. You might find numpy</span></span><br><span class="line"><span class="string">    functions np.exp, np.sum, np.reshape, np.max, and numpy</span></span><br><span class="line"><span class="string">    broadcasting useful for this task.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Numpy broadcasting documentation:</span></span><br><span class="line"><span class="string">    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    You should also make sure that your code works for a single</span></span><br><span class="line"><span class="string">    N-dimensional vector (treat the vector as a single row) and</span></span><br><span class="line"><span class="string">    for M x N matrices. This may be useful for testing later. Also,</span></span><br><span class="line"><span class="string">    make sure that the dimensions of the output match the input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    You must implement the optimization in problem 1(a) of the</span></span><br><span class="line"><span class="string">    written assignment!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A N dimensional vector or M x N dimensional numpy matrix.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    x -- You are allowed to modify x in-place</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    orig_shape = x.shape</span><br><span class="line">    ndim = x.ndim</span><br><span class="line">    max = np.max(x, axis=ndim<span class="number">-1</span>, keepdims=<span class="keyword">True</span>)   <span class="comment"># max value of array of each row. (M x 1)</span></span><br><span class="line">    exp = np.exp(x - max)                         <span class="comment"># exp of each element. (M x N)</span></span><br><span class="line">    sum = np.sum(exp, axis=ndim<span class="number">-1</span>, keepdims=<span class="keyword">True</span>) <span class="comment"># sum of each row. (M x 1)</span></span><br><span class="line">    x = exp / sum                                 <span class="comment"># softmax. (M x N)</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape == orig_shape</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="2-Neural-Network-Basics-30-points"><a href="#2-Neural-Network-Basics-30-points" class="headerlink" title="2 Neural Network Basics (30 points)"></a>2 Neural Network Basics (30 points)</h2><p><strong>(a)</strong> (3 points) Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the function value (i.e., in some expression where only σ(x), but not x, is present). Assume that the input x is a scalar for this question. Recall, the sigmoid function is</p>
<p>$$σ(x) = \frac{1}{1+e^{−x}}$$</p>
<p><strong>Solution</strong></p>
<p>应用链式求导法则</p>
<p>$$<br>\begin{align}<br>\frac {\partial{σ}} {\partial{x}} =&amp; \frac {\partial \frac{1}{1+e^{−x}}}{\partial x} \<br>    =&amp; \frac {\partial \frac{1}{1+e^{−x}}}{\partial (1+e^{−x})} \cdot \frac {\partial (1+e^{−x})}{\partial e^{−x}} \cdot \frac{\partial e^{−x}}{\partial (-x)} \cdot \frac{\partial (-x)}{\partial x} \<br>    =&amp; \frac{-1}{(1+e^{-x})^2} \cdot 1 \cdot e^{-x} \cdot -1 \<br>    =&amp; \frac{e^{-x}}{(1+e^{-x})^2} \<br>    =&amp; \frac{1 + e^{-x} - 1} {(1+e^{-x})^2} \<br>    =&amp; \frac{1}{1+e^{−x}} - \frac{1} {(1+e^{-x})^2} \<br>    =&amp; σ - σ^2 \<br>    =&amp; σ(1 - σ) \<br>\end{align}<br>$$</p>
<p><strong>(b)</strong> (3 points) Derive the gradient with regard to the inputs of a softmax function when cross entropy loss is used for evaluation, i.e., find the gradients with respect to the softmax input vector θ, when the prediction is made by $\hat{y} = softmax(θ)$. Remember the cross entropy function is</p>
<p>$$CE(y,\hat{y}) = − \sum_i y_i \log(\hat{y}_i)$$</p>
<p>where y is the one-hot label vector, and yˆ is the predicted probability vector for all classes. (Hint: you might want to consider the fact many elements of y are zeros, and assume that only the k-th dimension of y is one.)</p>
<p><strong>Solution</strong><br>已知$\hat{y}_i = \frac{e^{\theta_i}}{\sum_j{e^{\theta_j}}}$且y的为one-hot向量。设y的第k个元素为1，其他元素为0，可以得到：<br>$$<br>\begin{align}<br>CE(\theta) =&amp; − \sum_i y_i \log(\hat{y}_i) \<br>    =&amp; - (0 + y_k\log(\hat{y}_k) + 0) \<br>    =&amp; - \log(\hat{y}_k) \<br>    =&amp; - \log(\frac{e^{\theta_k}}{\sum_j{e^{\theta_j}}}) \<br>    =&amp; - \theta_k + \log(\sum_j{e^{\theta_j}})<br>\end{align}<br>$$</p>
<p>$$\frac{\partial{CE}}{\partial{\theta}}=- \frac{\theta_k}{\partial{\theta}} + \frac{\partial{\log(\sum_j{e^{\theta_j}})}}{\partial{\theta}}$$</p>
<p>对于第一部分，对向量求导是先对向量中的元素分别求导，再将结果组成向量，维度保持不变。其中， $\frac{\partial{\theta_k}}{\partial{\theta_k}}=1$ 且当 $j \neq k$时$\frac{\partial{\theta_k}}{\partial{\theta_j}}=0$ , 可以得到：</p>
<p>$$\frac{\partial{\theta_k}}{\partial{\theta}}=y$$</p>
<p>对于第二部分，应用链式求导法则，对单个元素进行求导：<br>$$<br>\begin{align}<br>\frac{\partial{\log(\sum_j{e^{\theta_j})}}}{\partial{\theta_i}}<br>    &amp;= \frac{\partial{\log(\sum_j{e^{\theta_j})}}}{\sum_j{e^{\theta_j}}} \cdot \frac{\sum_j{e^{\theta_j}}}{\partial{\theta_i}} \<br>    &amp;= \frac{1}{\sum_j{e^{\theta_j}}} \cdot e^{\theta_i} \<br>    &amp;= \frac{e^{\theta_i}}{\sum_j{e^{\theta_j}}} \<br>    &amp;= \hat{y}_i \<br>\end{align}<br>$$<br>将第一部分和第二部分组合在一起：<br>$$\frac{\partial{CE}}{\partial{\theta}}=- \frac{\theta_k}{\partial{\theta}} + \frac{\partial{\log(\sum_j{e^{\theta_j}})}}{\partial{\theta}}=-y + \hat{y}$$</p>
<p><strong>(c)</strong> (6 points) Derive the gradients with respect to the <em>inputs</em> x to an one-hidden-layer neural network (that is,find $\frac{\partial{J}}{\partial{x}}$ where $J=CE(y,yˆ)$ is the cost function for the neural network).The neural network employs sigmoid activation function for the hidden layer, and softmax for the output layer. Assume the one-hot label vector is y, and cross entropy cost is used. (Feel free to use σ′(x) as the shorthand for sigmoid gradient, and feel free to define any variables whenever you see fit.)</p>
<p><img src="/media/cs224n-assignment-1/one-hidden-layer-nn.png" alt="one-hidden-layer-nn"></p>
<p>   Recall that the forward propagation is as follows </p>
<p>$$h = sigmoid(xW_1 + b_1)$$<br>$$\hat{y} = softmax(hW_2 + b_2) $$</p>
<p>Note that here we’re assuming that the input vector (thus the hidden variables and output probabilities) is a row vector to be consistent with the programming assignment. When we apply the sigmoid function to a vector, we are applying it to each of the elements of that vector. <strong>$W_i$</strong> and <strong>$b_i$</strong> (i = 1, 2) are the weights and biases, respectively, of the two layers.</p>
<p><strong>Solution</strong><br>设<br>$$z_1 = xW_1 + b_1$$<br>$$z_2 = hW_2 + b_2$$</p>
<p>已知<br>$$h = sigmoid(xW_1 + b_1) = sigmoid(z_1)$$<br>$$\hat{y} = softmax(hW_2 + b_2) = softmax(z_2)$$<br>$$CE(y,\hat{y}) = − \sum_i y_i \log(\hat{y}_i)$$</p>
<p>可以得到：<br>$$<br>\begin{align}<br>\frac{\partial{J}}{\partial{x}} =&amp; \frac{\partial{CE}}{\partial{x}} \<br>    =&amp; \frac{\partial{CE}}{\partial{z_2}} \cdot \frac{\partial{z_2}}{\partial{h}} \cdot \frac{\partial{h}}{\partial{z_1}} \cdot \frac{\partial{z_1}}{\partial{x}} \<br>    =&amp; (\hat{y} - y) \cdot W_2 \cdot (z_1 - z_1^2) \cdot W_1<br>\end{align}<br>$$</p>
<p><strong>(d)</strong> (2 points) How many parameters are there in this neural network, assuming the input is Dx-dimensional, the output is Dy-dimensional, and there are H hidden units?</p>
<p><strong>Solution</strong></p>
<p>$$SUM = H \cdot (D_x + 1) + D_y \cdot (H + 1)$$</p>
<p><strong>(e)</strong> (4 points) Fill in the implementation for the sigmoid activation function and its gradient in <code>q2_sigmoid.py</code>. Test your implementation using <code>python q2_sigmoid.py</code>. <em>Again, thoroughly test your code as the provided tests may not be exhaustive.</em></p>
<p><strong>Solution</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid function for the input here.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_grad</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the gradient for the sigmoid function here. Note that</span></span><br><span class="line"><span class="string">    for this implementation, the input s should be the sigmoid</span></span><br><span class="line"><span class="string">    function value of your original input x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    s -- A scalar or numpy array.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    ds -- Your computed gradient.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ds = s * (<span class="number">1</span> - s)</span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure>
<p><strong>(f)</strong> (4 points) To make debugging easier, we will now implement a gradient checker. Fill in the implementation for gradcheck naive in <code>q2_gradcheck.py</code>. Test your code using <code>python q2_gradcheck.py</code>.</p>
<p><strong>Solution</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First implement a gradient checker by filling in the following functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradcheck_naive</span><span class="params">(f, x)</span>:</span></span><br><span class="line">    <span class="string">""" Gradient check for a function f.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    f -- a function that takes a single argument and outputs the</span></span><br><span class="line"><span class="string">         cost and its gradients</span></span><br><span class="line"><span class="string">    x -- the point (numpy array) to check the gradient at</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    rndstate = random.getstate()</span><br><span class="line">    random.setstate(rndstate)</span><br><span class="line">    fx, grad = f(x) <span class="comment"># Evaluate function value at original point</span></span><br><span class="line">    h = <span class="number">1e-4</span>        <span class="comment"># Do not change this!</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Iterate over all indexes in x</span></span><br><span class="line">    it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line">        ix = it.multi_index</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Try modifying x[ix] with h defined above to compute</span></span><br><span class="line">        <span class="comment"># numerical gradients. Make sure you call random.setstate(rndstate)</span></span><br><span class="line">        <span class="comment"># before calling f(x) each time. This will make it possible</span></span><br><span class="line">        <span class="comment"># to test cost functions with built in randomness later.</span></span><br><span class="line">        store = x[ix]</span><br><span class="line">        x[ix] = store + h</span><br><span class="line">        random.setstate(rndstate)</span><br><span class="line">        a = f(x)[<span class="number">0</span>]</span><br><span class="line">        x[ix] = store - h</span><br><span class="line">        random.setstate(rndstate)</span><br><span class="line">        b = f(x)[<span class="number">0</span>]</span><br><span class="line">        x[ix] = store</span><br><span class="line">        numgrad = (a - b) / (<span class="number">2</span> * h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compare gradients</span></span><br><span class="line">        reldiff = abs(numgrad - grad[ix]) / max(<span class="number">1</span>, abs(numgrad), abs(grad[ix]))</span><br><span class="line">        <span class="keyword">if</span> reldiff &gt; <span class="number">1e-5</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Gradient check failed."</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">"First gradient error found at index %s"</span> % str(ix)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Your gradient: %f \t Numerical gradient: %f"</span> % (</span><br><span class="line">                grad[ix], numgrad)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        it.iternext() <span class="comment"># Step to next dimension</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Gradient check passed!"</span></span><br></pre></td></tr></table></figure>
<p><strong>(g)</strong> (8 points) Now, implement the forward and backward passes for a neural network with one sigmoid hidden layer. Fill in your implementation in <code>q2_neural.py</code>. Sanity check your implementation with <code>python q2_neural.py</code>.</p>
<p><strong>Solution</strong></p>
<p>已知<br>$$CE(y, \hat{y}) = \sum_i{y_i\log{\hat{y}_i}}$$<br>$$\hat{y} = a_2 = softmax(z_2) = \frac{exp(z2)}{\sum_i{exp(z_{2i})}}$$<br>$$z_2=a_1W_2 + b_2$$<br>$$a_1 = sigmoid(z_1)$$<br>$$z_1 = a_0W_1 + b_1$$<br>$$a_0 = x$$<br>$$\frac{\partial{CE}}{\partial{z2}} = gradz2 = -y + \hat{y}$$</p>
<p>对W2求偏导，gradW2:<br>$$<br>\begin{align}<br>\frac{\partial{CE}}{\partial{W_2}} =&amp; \frac{\partial{CE}}{\partial{z_2}} \cdot \frac{\partial{z_2}}{\partial{W_2}} \<br>    =&amp; a_1^T \cdot gradz2<br>\end{align}<br>$$</p>
<p>对b2求导，gradb2:<br>$$<br>\begin{align}<br>\frac{\partial{CE}}{\partial{b_2}} =&amp; \frac{\partial{CE}}{\partial{z_2}} \cdot \frac{\partial{z_2}}{\partial{b_2}} \<br>    =&amp; gradz2<br>\end{align}<br>$$</p>
<p>对a1求导，grada1:<br>$$<br>\begin{align}<br>\frac{\partial{CE}}{\partial{a_1}} =&amp; \frac{\partial{CE}}{\partial{z_2}} \cdot \frac{\partial{z_2}}{\partial{a_1}} \<br>    =&amp; W_2^T \cdot gradz2<br>\end{align}<br>$$</p>
<p>对z1求导，gradz1:<br>$$<br>\begin{align}<br>\frac{\partial{CE}}{\partial{z_1}} =&amp; \frac{\partial{CE}}{\partial{a_1}} \cdot \frac{\partial{a_1}}{\partial{z_1}} \<br>    =&amp; sigmoid’(a_1) \cdot grada1 \<br>\end{align}<br>$$</p>
<p>对W1求导，gradW1:<br>$$<br>\begin{align}<br>\frac{\partial{CE}}{\partial{W_1}} =&amp; \frac{\partial{CE}}{\partial{z_1}} \cdot \frac{\partial{z_1}}{\partial{W_1}} \<br>    =&amp; a_0^T \cdot gradz1 \<br>    =&amp; x^T \cdot gradz1 \<br>\end{align}<br>$$</p>
<p>对b1求导，gradb1:<br>$$<br>\begin{align}<br>\frac{\partial{CE}}{\partial{b_1}} =&amp; \frac{\partial{CE}}{\partial{z_1}} \cdot \frac{\partial{z_1}}{\partial{b_1}} \<br>    =&amp; gradz1<br>\end{align}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_backward_prop</span><span class="params">(data, labels, params, dimensions)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward and backward propagation for a two-layer sigmoidal network</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Compute the forward propagation and for the cross entropy cost,</span></span><br><span class="line"><span class="string">    and backward propagation for the gradients for all parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    data -- M x Dx matrix, where each row is a training example.</span></span><br><span class="line"><span class="string">    labels -- M x Dy matrix, where each row is a one-hot vector.</span></span><br><span class="line"><span class="string">    params -- Model parameters, these are unpacked for you.</span></span><br><span class="line"><span class="string">    dimensions -- A tuple of input dimension, number of hidden units</span></span><br><span class="line"><span class="string">                  and output dimension</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Unpack network parameters (do not modify)</span></span><br><span class="line">    ofs = <span class="number">0</span></span><br><span class="line">    Dx, H, Dy = (dimensions[<span class="number">0</span>], dimensions[<span class="number">1</span>], dimensions[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    W1 = np.reshape(params[ofs:ofs + Dx * H], (Dx, H))      <span class="comment"># Dx * H</span></span><br><span class="line">    ofs += Dx * H</span><br><span class="line">    b1 = np.reshape(params[ofs:ofs + H], (<span class="number">1</span>, H))            <span class="comment"># 1 * H</span></span><br><span class="line">    ofs += H</span><br><span class="line">    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))      <span class="comment"># H * Dy</span></span><br><span class="line">    ofs += H * Dy</span><br><span class="line">    b2 = np.reshape(params[ofs:ofs + Dy], (<span class="number">1</span>, Dy))</span><br><span class="line"></span><br><span class="line">    x = data                                                <span class="comment"># M * Dx</span></span><br><span class="line">    y = labels                                              <span class="comment"># M * Dy</span></span><br><span class="line">    M = x.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### forward propagation</span></span><br><span class="line">    z1 = np.dot(x, W1) + b1</span><br><span class="line">    a1 = sigmoid(z1)</span><br><span class="line"></span><br><span class="line">    z2 = np.dot(a1, W2) + b2</span><br><span class="line">    y_hat = a2 = softmax(z2)</span><br><span class="line"></span><br><span class="line">    cost = -np.sum(np.log(a2[np.arange(M), np.argmax(y, axis=<span class="number">1</span>)])) <span class="comment"># Cross Entropy</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### backward propagation</span></span><br><span class="line">    gradz2 = y_hat - y</span><br><span class="line">    gradW2 = np.dot(a1.T, gradz2)</span><br><span class="line">    gradb2 = np.sum(gradz2, axis=<span class="number">0</span>)</span><br><span class="line">    grada2 = np.dot(gradz2, W2.T)</span><br><span class="line">    gradz1 = sigmoid_grad(a1) * grada2</span><br><span class="line">    gradW1 = np.dot(x.T, gradz1)</span><br><span class="line">    gradb1 = np.sum(gradz1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Stack gradients (do not modify)</span></span><br><span class="line">    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),</span><br><span class="line">        gradW2.flatten(), gradb2.flatten()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost, grad</span><br></pre></td></tr></table></figure>
<h2 id="3-word2vec-40-points-2-bonus"><a href="#3-word2vec-40-points-2-bonus" class="headerlink" title="3 word2vec (40 points + 2 bonus)"></a>3 word2vec (40 points + 2 bonus)</h2><p><strong>(a)</strong> (3 points) Assume you are given a predicted word vector vc corresponding to the center word c for skipgram, and word prediction is made with the softmax function found in word2vec models</p>
<p>$$\hat{y}_o=p(o \mathbin{\vert} c)=\frac{e^{u_o^T v_c}}{\sum_{w=1}^W{e^{u_w^T v_c}}}$$</p>
<p>where w denotes the w-th word and $u_w$ (w = 1, . . . , W ) are the “output” word vectors for all words in the vocabulary. Assume cross entropy cost is applied to this prediction and word o is the expected word (the o-th element of the one-hot label vector is one), derive the gradients with respect to $v_c$.</p>
<p><em>Hint: It will be helpful to use notation from question 2. For instance, letting y_hat be the vector of softmax predictions for every word, y as the expected word vector, and the loss function</em> </p>
<p>$$J_{softmax−CE}(o,vc,U) = CE(y,yˆ)$$</p>
<p>where $U = [u_1 , u_2 , · · · , u_W]$ is the matrix of all the output vectors. Make sure you state the orientation of your vectors and matrices.</p>
<p><strong>Solution 1:</strong></p>
<p>设$ z_i = u_i^T v_c $，则：</p>
<p>$$ z = U^Tv_c $$</p>
<p>$$<br>\begin{align}<br>J =&amp; - \sum_{i=1}^W{y_i\log(\frac{exp(u_i^T v_c)}{\sum_{w=1}^{W}{exp(u_w^T v_c)}})} \<br>    =&amp; - \sum_{i=1}^W{y_i\log(\frac{exp(z_i)}{\sum_{w=1}^{W}{exp(z_w)}})}<br>\end{align}<br>$$</p>
<p>可以得到:</p>
<p>$$<br>\begin{align}<br>\frac{\partial{J}}{\partial{v_c}} =&amp; \frac{\partial{CE}}{\partial{z}}  \cdot \frac{\partial{z}}{\partial{v_c}} \<br>    =&amp;\frac{\partial{CE}}{\partial{z}} \cdot \frac{\partial{U^Tv_c}}{\partial{v_c}} \<br>    =&amp; U^T \cdot (\hat{y} - y)<br>\end{align}<br>$$</p>
<p><strong>Solution 2:</strong></p>
<ol>
<li>|W|是词汇表中的单词数量</li>
<li>y和$\hat{y}$是|W| x 1的列向量</li>
<li>$u_i$和$v_j$是D x 1的列向量(D输入输出的向量的维度)</li>
<li>y是|W| x 1的one-hot编码列向量 </li>
<li>$\hat{y}$是|W| x 1的softmax输出的列向量 </li>
<li>y hat: $\hat{y_i} = P(i|c) = \frac {exp(u_i^T v_c)} { \sum_{w=1}^W{exp(u_w^T v_c)} }$</li>
<li>交叉熵损失函数: $J = -\sum_{i=1}^Wy_ilog({\hat{y_i}})$</li>
<li>$U = [u_1, u_2, …,u_k, …u_W]$是由$u_k$列向量组成的矩阵</li>
</ol>
<p>损失函数，</p>
<p>$$J = - \sum_{i=1}^W y_i log(\frac{exp(u_i^Tv_c)}{\sum_{w=1}^Wexp(u_w^Tv_c)})$$</p>
<p>简化,</p>
<p>$$J = - \sum_{i=1}^Wy_i[u_i^Tv_c - log(\sum_{w=1}^Wexp(u_w^Tv_c))]$$</p>
<p>由于y是one-hot编码的列向量，除了第k个元素之外的其他元素都是0。也就是说，上面的求和公式中，只有第k个是非0的，其他的元素均为0。所以，损失函数可以写为：</p>
<p>$$J = -y_k[u_k^Tv_c - log(\sum_{w=1}^Wexp(u_w^Tv_c))]$$</p>
<p>注意, 其中$y_k$为1。求解$\frac{\partial{J}}{\partial{v_c}}$:</p>
<p>$$\frac{\partial J}{\partial v_c} = -[u_k - \frac{\sum_{w=1}^Wexp(u_w^Tv_c)u_w}{\sum_{x=1}^Wexp(u_x^Tv_c)}]$$</p>
<p>使用定义(5)，我们可以重写上述公式：</p>
<p>$$\frac{\partial J}{\partial v_c} = \sum_{w=1}^W (\hat{y}_w u_w) - u_k$$</p>
<p>现在我们将公式使用矩阵形式重写：</p>
<ol>
<li>$u_k$可以被写为矩阵向量相乘: $U \cdot y$</li>
<li>且$\sum_{w=1}^W (\hat{y}_w u_w)$是矩阵U中的向量$u_w$按照比例$\hat{y}_w$缩放的的一个线性转换。</li>
</ol>
<p>所以，公式整体可以写成：$U[\hat{y} - y]$</p>
<p><strong>(b)</strong> (3 points) As in the previous part, derive gradients for the “output” word vectors $u_w$’s (including $u_o$).</p>
<p><strong>Solution:</strong></p>
<p>$$<br>\begin{align}<br>\frac{\partial{J}}{\partial{U}} =&amp; \frac{\partial{CE}}{\partial{z}} \cdot \frac{\partial{z}}{\partial{U}} \<br>    =&amp; v_c \cdot (\hat{y} - y)^T<br>\end{align}<br>$$</p>
<p><strong>(c)</strong> (6 points) Repeat part (a) and (b) assuming we are using the negative sampling loss for the predicted vector vc, and the expected output word is o. Assume that K negative samples (words) are drawn, and they are 1, · · · , K, respectively for simplicity of notation ($o \notin {1, . . . , K}$). Again, for a given word, o, denote its output vector as $u_o$. The negative sampling loss function in this case is K </p>
<p>$$J_{neg−sample}(o,v_c,U)=−\log(σ(u^⊤_o v_c)) − \sum_{k=1}^{K}\log(σ(−u^⊤_k v_c))$$</p>
<p>where σ(·) is the sigmoid function.<br>After you’ve done this, describe with one sentence why this cost function is much more efficient to compute than the softmax-CE loss (you could provide a speed-up ratio, i.e., the runtime of the softmax- CE loss divided by the runtime of the negative sampling loss).<br><em>Note: the cost function here is the negative of what Mikolov et al had in their original paper, because we are doing a minimization instead of maximization in our code.</em></p>
<p><strong>Solution:</strong></p>
<p>对vc求偏导:<br>$$<br>\begin{align}<br>\frac{\partial{J}}{\partial{v_c}} =&amp; - \frac{1}{σ(u_o^T v_c)} \cdot σ(u_o^T v_c)(1-σ(u_o^T v_c)) \cdot u_o - \sum_{k=1}^{K}{\frac{1}{σ(-u_k^T v_c)} \cdot σ(-u_k^T v_c)(1-σ(-u_k^T v_c)) \cdot -u_k} \<br>    =&amp; - (1 - σ(u_o^T v_c)) \cdot u_o - \sum_{k=1}^{K}{(1 - σ(-u_k^T v_c)) \cdot -u_k} \<br>    =&amp; (σ(u_o^T v_c) - 1) \cdot u_o - \sum_{k=1}^{K}{(σ(-u_k^T v_c) - 1) \cdot u_k}<br>\end{align}<br>$$</p>
<p>对uo求偏导:<br>$$<br>\begin{align}<br>\frac{\partial{J}}{\partial{u_o}} =&amp; - \frac{1}{σ(u_o^T v_c)} \cdot σ(u_o^T v_c)(1-σ(u_o^T v_c)) \cdot v_c - 0 \<br>    =&amp; (σ(u_o^T v_c) - 1) \cdot v_c<br>\end{align}<br>$$</p>
<p>对uk求偏导:<br>$$<br>\begin{align}<br>\frac{\partial{J}}{\partial{u_k}} =&amp; - 0 - \frac{1}{σ(-u_k^T v_c)} \cdot σ(-u_k^T v_c)(1-σ(-u_k^T v_c)) \cdot - v_c \<br>    =&amp; - (σ(-u_k^T v_c) - 1) \cdot v_c<br>\end{align}<br>$$</p>
<p><strong>(d)</strong> (8 points) Derive gradients for all of the word vectors for skip-gram and CBOW given the previous parts and given a set of context words $$ word_{c-m}, … , word_{c-1}, word_{c}, word_{c+1}, … word_{c+m} $$, where m is the context size. Denote the “input” and “output” word vectors for $word_k$ as $v_k$ and $u_k$ respectively.</p>
<p><em>Hint: feel free to use $$F(o, v_c)$$ (where <strong>o</strong> is the expected word) as a placeholder for the $$J_{softmax−CE}(o, v_c , …)$$ or $$J_{neg−sample}(o,v_c,…)$$ cost functions in this part — you’ll see that this is a useful abstraction for the coding part. That is, your solution may contain terms of the form $$\frac{\partial{F(o,v_c)}}{\partial{…}}$$</em></p>
<p>Recall that for skip-gram, the cost for a context centered around c is<br>$$J_{skip-gram}(word_{c−m…c+m}) = \sum_{-m \le j \le m, j \ne 0}F(w_{c+j},v_c)$$<br>where $w_{c+j}$ refers to the word at the j-th index from the center.<br>CBOW is slightly different. Instead of using $v_c$ as the predicted vector, we use $\hat{v}$ defined below. For (a simpler variant of) CBOW, we sum up the input word vectors in the context<br>$$\hat{v} = \sum_{-m \le j \le m, j \ne 0}{v_{c+j}}$$<br>then the CBOW cost is<br>$$J_{CBOW}(word_{c−m…c+m}) = F(w_c, \hat{v})$$<br><em>Note: To be consistent with the $\hat{v}$ notation such as for the code portion, for skip-gram $\hat{v} = v_c$.</em></p>
<p><strong>Solution:</strong></p>
<p>假设U是由词汇表中所有单词的输出向量组成的矩阵. </p>
<p>已知$\frac{\partial{F(w_j, \hat{v})}}{\partial{U}}$和$\frac{\partial{F(w_j, \hat{v}}}{\partial{\hat{v}}}$</p>
<p>对于skip-gram模型进行求偏导，对U求偏导:<br>$$<br>\frac{\partial{J_{skip-gram}(word_{c-m…c+m})}}{\partial{U}} = \sum_{-m \le j \le m, j \ne 0}\frac{\partial{F(w_{c+j}, \hat{v})}}{\partial{U}}<br>$$</p>
<p>对vc求偏导:<br>$$<br>\frac{\partial{J_{skip-gram}(word_{c-m…c+m})}}{\partial{v_c}} = \sum_{-m \le j \le m, j \ne 0}\frac{\partial{F(w_{c+j}, \hat{v})}}{\partial{v_c}}<br>$$</p>
<p>对vj求偏导:<br>$$<br>\frac{\partial{J_{skip-gram}(word_{c-m…c+m})}}{\partial{v_j}} = 0,\ for\ all\ j \ne c<br>$$</p>
<p>对CBOW模型进行求偏导，对U求偏导:<br>$$<br>\frac{\partial{J_{CBOW}(word_{c-m…c+m})}}{\partial{U}} = \frac{\partial{F(w, \hat{v})}}{\partial{U}}<br>$$</p>
<p>对v hat求偏导:<br>$$<br>\frac{\partial{J_{CBOW}(word_{c-m…c+m})}}{\partial{v_j}} = \frac{\partial{F(w, \hat{v})}}{\partial{\hat{v}}}, for\ all\ j \in {c-m, … , c+m}<br>$$</p>
<p>对vj求偏导:<br>$$<br>\frac{\partial{J_{CBOW}(word_{c-m…c+m})}}{\partial{v_j}} = 0, for\ all\ j \notin {c-m, … , c+m}<br>$$</p>
<p><strong>(e)</strong> (12 points) In this part you will implement the word2vec models and train your own word vectors with stochastic gradient descent (SGD). First, write a helper function to normalize rows of a matrix in q3_word2vec.py. In the same file, fill in the implementation for the softmax and negative sampling cost and gradient functions. Then, fill in the implementation of the cost and gradient functions for the skip-gram model. When you are done, test your implementation by running python q3_word2vec.py. <em>Note: If you choose not to implement CBOW (part h), simply remove the NotImplementedError so that your tests will complete.</em></p>
<p><strong>Solution:</strong></p>
<p>代码: <a href="https://github.com/amendgit/cs224n/blob/master/assignment1/code/q3_word2vec.py" target="_blank" rel="noopener">q3_word2vec.py</a></p>
<p>normlize:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeRows</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">""" Row normalization function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement a function that normalizes each row of a matrix to have</span></span><br><span class="line"><span class="string">    unit length.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    y = np.linalg.norm(x, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    x = x / y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>softmax:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmaxCostAndGradient</span><span class="params">(predicted, target, outputVectors, dataset)</span>:</span></span><br><span class="line">    <span class="string">""" Softmax cost function for word2vec models</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the cost and gradients for one predicted word vector</span></span><br><span class="line"><span class="string">    and one target word vector as a building block for word2vec</span></span><br><span class="line"><span class="string">    models, assuming the softmax prediction function and cross</span></span><br><span class="line"><span class="string">    entropy loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    predicted -- numpy ndarray, predicted word vector (\hat&#123;v&#125; in</span></span><br><span class="line"><span class="string">                 the written component)</span></span><br><span class="line"><span class="string">    target -- integer, the index of the target word</span></span><br><span class="line"><span class="string">    outputVectors -- "output" vectors (as rows) for all tokens</span></span><br><span class="line"><span class="string">    dataset -- needed for negative sampling, unused here.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- cross entropy cost for the softmax word prediction</span></span><br><span class="line"><span class="string">    gradPred -- the gradient with respect to the predicted word</span></span><br><span class="line"><span class="string">           vector</span></span><br><span class="line"><span class="string">    grad -- the gradient with respect to all the other word</span></span><br><span class="line"><span class="string">           vectors</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    We will not provide starter code for this function, but feel</span></span><br><span class="line"><span class="string">    free to reference the code you previously wrote for this</span></span><br><span class="line"><span class="string">    assignment!</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grad = np.zeros_like(outputVectors)</span><br><span class="line">    gradPred = np.zeros_like(predicted)</span><br><span class="line"></span><br><span class="line">    prob = softmax(np.dot(predicted, outputVectors.T))</span><br><span class="line">    cost = - np.log(prob[target])</span><br><span class="line"></span><br><span class="line">    gradZ = prob</span><br><span class="line">    gradZ[target] -= <span class="number">1</span> <span class="comment"># y_hat - y</span></span><br><span class="line"></span><br><span class="line">    N, D = outputVectors.shape</span><br><span class="line"></span><br><span class="line">    grad     = np.dot(gradZ.reshape(N, <span class="number">1</span>), predicted.reshape(<span class="number">1</span>, D))</span><br><span class="line">    gradPred = (np.dot(gradZ.reshape(<span class="number">1</span>, N), outputVectors)).flatten()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost, gradPred, grad</span><br></pre></td></tr></table></figure>
<p>negative sampling:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">negSamplingCostAndGradient</span><span class="params">(predicted, target, outputVectors, dataset,</span></span></span><br><span class="line"><span class="function"><span class="params">                               K=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Negative sampling cost function for word2vec models</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the cost and gradients for one predicted word vector</span></span><br><span class="line"><span class="string">    and one target word vector as a building block for word2vec</span></span><br><span class="line"><span class="string">    models, using the negative sampling technique. K is the sample</span></span><br><span class="line"><span class="string">    size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: See test_word2vec below for dataset's initialization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments/Return Specifications: same as softmaxCostAndGradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Sampling of indices is done for you. Do not modify this if you</span></span><br><span class="line">    <span class="comment"># wish to match the autograder and receive points!</span></span><br><span class="line">    indices = [target]</span><br><span class="line">    indices.extend(getNegativeSamples(target, dataset, K))</span><br><span class="line"></span><br><span class="line">    U = outputVectors</span><br><span class="line">    vc = predicted</span><br><span class="line"></span><br><span class="line">    grad = np.zeros_like(U)</span><br><span class="line">    gradPred = np.zeros_like(vc)</span><br><span class="line"></span><br><span class="line">    N, D = U.shape</span><br><span class="line"></span><br><span class="line">    labels = np.array([<span class="number">1</span>] + [<span class="number">-1</span> <span class="keyword">for</span> k <span class="keyword">in</span> range(K)])</span><br><span class="line">    u = U[indices] <span class="comment"># uo, uk1, uk2, ... , uK</span></span><br><span class="line"></span><br><span class="line">    z = np.dot(u, vc) * labels</span><br><span class="line">    probs = sigmoid(z)</span><br><span class="line">    cost = - np.sum(np.log(probs))</span><br><span class="line"></span><br><span class="line">    gradZ    = labels * (probs - <span class="number">1</span>)</span><br><span class="line">    gradPred = gradZ.reshape(<span class="number">1</span>, K+<span class="number">1</span>).dot(u).flatten()</span><br><span class="line">    gradu    = gradZ.reshape(K+<span class="number">1</span>, <span class="number">1</span>).dot(vc.reshape(<span class="number">1</span>, vc.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(K+<span class="number">1</span>):</span><br><span class="line">        grad[indices[k]] += gradu[k,:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost, gradPred, grad</span><br></pre></td></tr></table></figure>
<p>skip-gram</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">skipgram</span><span class="params">(currentWord, C, contextWords, tokens, inputVectors, outputVectors,</span></span></span><br><span class="line"><span class="function"><span class="params">             dataset, word2vecCostAndGradient=softmaxCostAndGradient)</span>:</span></span><br><span class="line">    <span class="string">""" Skip-gram model in word2vec</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the skip-gram model in this function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    currrentWord -- a string of the current center word</span></span><br><span class="line"><span class="string">    C -- integer, context size</span></span><br><span class="line"><span class="string">    contextWords -- list of no more than 2*C strings, the context words</span></span><br><span class="line"><span class="string">    tokens -- a dictionary that maps words to their indices in</span></span><br><span class="line"><span class="string">              the word vector list</span></span><br><span class="line"><span class="string">    inputVectors -- "input" word vectors (as rows) for all tokens</span></span><br><span class="line"><span class="string">    outputVectors -- "output" word vectors (as rows) for all tokens</span></span><br><span class="line"><span class="string">    word2vecCostAndGradient -- the cost and gradient function for</span></span><br><span class="line"><span class="string">                               a prediction vector given the target</span></span><br><span class="line"><span class="string">                               word vectors, could be one of the two</span></span><br><span class="line"><span class="string">                               cost functions you implemented above.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- the cost function value for the skip-gram model</span></span><br><span class="line"><span class="string">    grad -- the gradient with respect to the word vectors</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    gradIn = np.zeros(inputVectors.shape)</span><br><span class="line">    gradOut = np.zeros(outputVectors.shape)</span><br><span class="line"></span><br><span class="line">    cIndex = tokens[currentWord]</span><br><span class="line">    predicted = inputVectors[cIndex, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> contextWord <span class="keyword">in</span> contextWords:</span><br><span class="line">        target = tokens[contextWord]</span><br><span class="line">        cCost, cGradPred, cGrad = word2vecCostAndGradient(predicted, target, outputVectors, dataset)</span><br><span class="line">        cost += cCost</span><br><span class="line">        gradIn[cIndex,:] += cGradPred</span><br><span class="line">        gradOut += cGrad</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost, gradIn, gradOut</span><br></pre></td></tr></table></figure>
<p><strong>(f)</strong> (4 points) Complete the implementation for your SGD optimizer in q3_sgd.py. Test your implementation by running python q3_sgd.py.</p>
<p><strong>Solution:</strong></p>
<p>完整代码: <a href="https://github.com/amendgit/cs224n/blob/master/assignment1/code/q3_sgd.py" target="_blank" rel="noopener">q3_sgd.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(f, x0, step, iterations, postprocessing=None, useSaved=False,</span></span></span><br><span class="line"><span class="function"><span class="params">        PRINT_EVERY=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Stochastic Gradient Descent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the stochastic gradient descent method in this function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    f -- the function to optimize, it should take a single</span></span><br><span class="line"><span class="string">         argument and yield two outputs, a cost and the gradient</span></span><br><span class="line"><span class="string">         with respect to the arguments</span></span><br><span class="line"><span class="string">    x0 -- the initial point to start SGD from</span></span><br><span class="line"><span class="string">    step -- the step size for SGD</span></span><br><span class="line"><span class="string">    iterations -- total iterations to run SGD for</span></span><br><span class="line"><span class="string">    postprocessing -- postprocessing function for the parameters</span></span><br><span class="line"><span class="string">                      if necessary. In the case of word2vec we will need to</span></span><br><span class="line"><span class="string">                      normalize the word vectors to have unit length.</span></span><br><span class="line"><span class="string">    PRINT_EVERY -- specifies how many iterations to output loss</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    x -- the parameter value after SGD finishes</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Anneal learning rate every several iterations</span></span><br><span class="line">    ANNEAL_EVERY = <span class="number">20000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> useSaved:</span><br><span class="line">        start_iter, oldx, state = load_saved_params()</span><br><span class="line">        <span class="keyword">if</span> start_iter &gt; <span class="number">0</span>:</span><br><span class="line">            x0 = oldx</span><br><span class="line">            step *= <span class="number">0.5</span> ** (start_iter / ANNEAL_EVERY)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> state:</span><br><span class="line">            random.setstate(state)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        start_iter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    x = x0</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> postprocessing:</span><br><span class="line">        postprocessing = <span class="keyword">lambda</span> x: x</span><br><span class="line"></span><br><span class="line">    expcost = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> xrange(start_iter + <span class="number">1</span>, iterations + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Don't forget to apply the postprocessing after every iteration!</span></span><br><span class="line">        <span class="comment"># You might want to print the progress every few iterations.</span></span><br><span class="line"></span><br><span class="line">        cost, grad = f(x)</span><br><span class="line">        x = x - step * grad</span><br><span class="line">        x = postprocessing(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % PRINT_EVERY == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> expcost:</span><br><span class="line">                expcost = cost</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                expcost = <span class="number">.95</span> * expcost + <span class="number">.05</span> * cost</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"iter %d: %f"</span> % (iter, expcost)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % SAVE_PARAMS_EVERY == <span class="number">0</span> <span class="keyword">and</span> useSaved:</span><br><span class="line">            save_params(iter, x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iter % ANNEAL_EVERY == <span class="number">0</span>:</span><br><span class="line">            step *= <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><strong>(g)</strong> (4 points) Show time! Now we are going to load some real data and train word vectors with everything you just implemented! We are going to use the Stanford Sentiment Treebank (SST) dataset to train word vectors, and later apply them to a simple sentiment analysis task. You will need to fetch the datasets first. To do this, run sh get datasets.sh. There is no additional code to write for this part; just run python q3_run.py.<br><em>Note: The training process may take a long time depending on the efficiency of your implementation (an efficient implementation takes approximately an hour). Plan accordingly!<br>When the script finishes, a visualization for your word vectors will appear. It will also be saved as q3_word_vectors.png in your project directory. Include the plot in your homework write up. Briefly explain in at most three sentences what you see in the plot.</em></p>
<p><strong>Solution:</strong></p>
<p><img src="/media/q3_word_vectors-2.png" alt="q3_word_vectors"></p>
<p><strong>(h)</strong> (Extra credit: 2 points) Implement the CBOW model in q3_word2vec.py. Note: This part is optional but the gradient derivations for CBOW in part (d) are not!.</p>
<p><strong>Solution:</strong></p>
<p>continue bag of words:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cbow</span><span class="params">(currentWord, C, contextWords, tokens, inputVectors, outputVectors,</span></span></span><br><span class="line"><span class="function"><span class="params">         dataset, word2vecCostAndGradient=softmaxCostAndGradient)</span>:</span></span><br><span class="line">    <span class="string">"""CBOW model in word2vec</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the continuous bag-of-words model in this function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments/Return specifications: same as the skip-gram model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Extra credit: Implementing CBOW is optional, but the gradient</span></span><br><span class="line"><span class="string">    derivations are not. If you decide not to implement CBOW, remove</span></span><br><span class="line"><span class="string">    the NotImplementedError.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    gradIn = np.zeros(inputVectors.shape)</span><br><span class="line">    gradOut = np.zeros(outputVectors.shape)</span><br><span class="line"></span><br><span class="line">    cIndex = tokens[currentWord]</span><br><span class="line">    predicted = inputVectors[cIndex, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> contextWord <span class="keyword">in</span> contextWords:</span><br><span class="line">        target = tokens[contextWord]</span><br><span class="line">        cCost, cGradPred, cGrad = word2vecCostAndGradient(predicted, target, outputVectors, dataset)</span><br><span class="line">        cost += cCost</span><br><span class="line">        gradIn[cIndex,:] += cGradPred</span><br><span class="line">        gradOut += cGrad</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost, gradIn, gradOut</span><br></pre></td></tr></table></figure>
<h2 id="4-Sentiment-Analysis-20-points"><a href="#4-Sentiment-Analysis-20-points" class="headerlink" title="4 Sentiment Analysis (20 points)"></a>4 Sentiment Analysis (20 points)</h2><p>Now, with the word vectors you trained, we are going to perform a simple sentiment analysis. For each sentence in the Stanford Sentiment Treebank dataset, we are going to use the average of all the word vectors in that sentence as its feature, and try to predict the sentiment level of the said sentence. The sentiment level of the phrases are represented as real values in the original dataset, here we’ll just use five classes:<br>“very negative (−−)”, “negative (−)”, “neutral”, “positive (+)”, “very positive (++)”<br>which are represented by 0 to 4 in the code, respectively. For this part, you will learn to train a softmax classifier, and perform train/dev validation to improve generalization.</p>
<p><strong>(a)</strong> (2 points) Implement a sentence featurizer. A simple way of representing a sentence is taking the average of the vectors of the words in the sentence. Fill in the implementation in  q4_sentiment.py.</p>
<p><strong>Solution:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSentenceFeatures</span><span class="params">(tokens, wordVectors, sentence)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Obtain the sentence feature for sentiment analysis by averaging its</span></span><br><span class="line"><span class="string">    word vectors</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement computation for the sentence features given a sentence.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Inputs:</span></span><br><span class="line">    <span class="comment"># tokens -- a dictionary that maps words to their indices in</span></span><br><span class="line">    <span class="comment">#           the word vector list</span></span><br><span class="line">    <span class="comment"># wordVectors -- word vectors (each row) for all tokens</span></span><br><span class="line">    <span class="comment"># sentence -- a list of words in the sentence of interest</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment"># - sentVector: feature vector for the sentence</span></span><br><span class="line"></span><br><span class="line">    sentVector = np.zeros((wordVectors.shape[<span class="number">1</span>],))</span><br><span class="line"></span><br><span class="line">    listOfInd = [tokens[w] <span class="keyword">for</span> w <span class="keyword">in</span> sentence]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> listOfInd:</span><br><span class="line">        sentVector += wordVectors[i]</span><br><span class="line">    sentVector /= len(listOfInd)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> sentVector.shape == (wordVectors.shape[<span class="number">1</span>],)</span><br><span class="line">    <span class="keyword">return</span> sentVector</span><br></pre></td></tr></table></figure>
<p>regularization: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getRegularizationValues</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Try different regularizations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return a sorted list of values to try.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    values = <span class="keyword">None</span>   <span class="comment"># Assign a list of floats in the block below</span></span><br><span class="line">    values = np.logspace(<span class="number">-4</span>, <span class="number">2</span>, num=<span class="number">100</span>, base=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted(values)</span><br></pre></td></tr></table></figure>
<p><strong>(b)</strong> (1 points) Explain in at most two sentences why we want to introduce regularization when doing classi-fication (in fact, most machine learning tasks).</p>
<p><strong>Solution:</strong></p>
<p>避免在训练集上overfitting，而在测试集上效果过差。</p>
<p><strong>(c)</strong> (2 points) Fill in the hyperparameter selection code in q4_sentiment.py to search for the “optimal” regularization parameter. Attach your code for chooseBestModel to your written write-up. You should be able to attain at least 36.5% accuracy on the dev and test sets using the pretrained vectors in part (d).</p>
<p><strong>Solution:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestModel</span><span class="params">(results)</span>:</span></span><br><span class="line">    <span class="string">"""Choose the best model based on parameter tuning on the dev set</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    results -- A list of python dictionaries of the following format:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            "reg": regularization,</span></span><br><span class="line"><span class="string">            "clf": classifier,</span></span><br><span class="line"><span class="string">            "train": trainAccuracy,</span></span><br><span class="line"><span class="string">            "dev": devAccuracy,</span></span><br><span class="line"><span class="string">            "test": testAccuracy</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Your chosen result dictionary.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    bestResult = <span class="keyword">None</span></span><br><span class="line">    bestResult = max(results, key=<span class="keyword">lambda</span> model: model[<span class="string">"dev"</span>])</span><br><span class="line">    <span class="keyword">return</span> bestResult</span><br></pre></td></tr></table></figure>
<p><strong>(d)</strong> (3 points) Run python q4_sentiment.py –yourvectors to train a model using your word vectors from q3. Now, run python q4_sentiment.py –pretrained to train a model using pretrained GloVe vectors (on Wikipedia data). Compare and report the best train, dev, and test accuracies. Why do you think the pretrained vectors did better? Be specific and justify with 3 distinct reasons.</p>
<p><strong>Solution:</strong><br>Higher dimensional word vectors may encode more infomation. GloVe vectors were trained on a much larger corpus. GloVe vs Word2Vec.</p>
<p><strong>(e)</strong> (4 points) Plot the classification accuracy on the train and dev set with respect to the regularization value for the pretrained GloVe vectors, using a logarithmic scale on the x-axis. This should have been done automatically. Include q4_reg_acc.png in your homework write up. Briefly explain in at most three sentences what you see in the plot.</p>
<p><strong>Solution:</strong><br><img src="/media/q4_reg_v_acc.png" alt="q4_reg_v_ac"></p>
<p><strong>(f)</strong> (4 points) We will now analyze errors that the model makes (with pretrained GloVe vectors). When you ran python q4_sentiment.py –pretrained, two files should have been generated. Take a look at q4_dev_conf.png and include it in your homework writeup. Interpret the confusion matrix in at most three sentences.</p>
<p><strong>Solution:</strong><br><img src="/media/q4_dev_conf.png" alt="q4_dev_conf"></p>
<p><strong>(g)</strong> (4 points) Next, take a look at q4_dev_pred.txt. Choose 3 examples where your classifier made errors and briefly explain the error and what features the classifier would need to classify the example correctly (1 sentence per example). Try to pick examples with different reasons.</p>
<p><strong>Solution:</strong></p>

  </div>
</article>



  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="http://github.com/amendgit">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Softmax-10-points"><span class="toc-number">1.</span> <span class="toc-text">1 Softmax (10 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Neural-Network-Basics-30-points"><span class="toc-number">2.</span> <span class="toc-text">2 Neural Network Basics (30 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-word2vec-40-points-2-bonus"><span class="toc-number">3.</span> <span class="toc-text">3 word2vec (40 points + 2 bonus)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Sentiment-Analysis-20-points"><span class="toc-number">4.</span> <span class="toc-text">4 Sentiment Analysis (20 points)</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://www.amendgit.com/2017/cs224n-assignment-1/"><i class="fa fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://www.amendgit.com/2017/cs224n-assignment-1/&text=CS224N Assignment 1"><i class="fa fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://www.amendgit.com/2017/cs224n-assignment-1/&title=CS224N Assignment 1"><i class="fa fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://www.amendgit.com/2017/cs224n-assignment-1/&is_video=false&description=CS224N Assignment 1"><i class="fa fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=CS224N Assignment 1&body=Check out this article: http://www.amendgit.com/2017/cs224n-assignment-1/"><i class="fa fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://www.amendgit.com/2017/cs224n-assignment-1/&title=CS224N Assignment 1"><i class="fa fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://www.amendgit.com/2017/cs224n-assignment-1/&title=CS224N Assignment 1"><i class="fa fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://www.amendgit.com/2017/cs224n-assignment-1/&title=CS224N Assignment 1"><i class="fa fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://www.amendgit.com/2017/cs224n-assignment-1/&title=CS224N Assignment 1"><i class="fa fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://www.amendgit.com/2017/cs224n-assignment-1/&name=CS224N Assignment 1&description="><i class="fa fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
      <ul>
        <li id="toc"><a class="icon" href="#" onclick='$("#toc-footer").toggle();return false;'><i class="fa fa-list fa-lg" aria-hidden="true"></i> TOC</a></li>
        <li id="share"><a class="icon" href="#" onclick='$("#share-footer").toggle();return false;'><i class="fa fa-share-alt fa-lg" aria-hidden="true"></i> Share</a></li>
        <li id="top" style="display:none"><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a></li>
        <li id="menu"><a class="icon" href="#" onclick='$("#nav-footer").toggle();return false;'><i class="fa fa-bars fa-lg" aria-hidden="true"></i> Menu</a></li>
      </ul>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2018 amendgit
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="http://github.com/amendgit">Projects</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

</body>
</html>

<!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
<link rel="stylesheet" href="/lib/meslo-LG/styles.css">
<link rel="stylesheet" href="/lib/justified-gallery/justifiedGallery.min.css">


<!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-86660611-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Disqus Comments -->


